{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "#\n",
    "#   Bellman Equation\n",
    "#   source: https://www.datahubbs.com/reinforcement-learning-markov-decision-processes/\n",
    "#   source: https://towardsdatascience.com/understanding-markov-decision-process-the-framework-behind-reinforcement-learning-4b5166f3c5b4\n",
    "#   source: https://github.com/maximecb/gym-minigrid\n",
    "#   source: https://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2020-08-06-03-Policy-Gradient-With-Gym-MiniGrid.ipynb#scrollTo=gp_vT2vgyOoB\n",
    "#   source: https://github.com/maximecb/gym-minigrid\n",
    "#\n",
    "#\n",
    "#\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0  ->  -0.8\n",
      "Step  1  ->  -1.12\n",
      "Step  2  ->  -1.248\n",
      "Step  3  ->  -1.2992\n",
      "Step  4  ->  -1.31968\n",
      "Step  5  ->  -1.327872\n",
      "Step  6  ->  -1.3311488\n",
      "Step  7  ->  -1.33245952\n",
      "Step  8  ->  -1.3329838079999998\n",
      "Step  9  ->  -1.3331935231999998\n",
      "Step  10  ->  -1.33327740928\n",
      "Step  11  ->  -1.333310963712\n",
      "Step  12  ->  -1.3333243854847998\n",
      "Step  13  ->  -1.3333297541939197\n",
      "Step  14  ->  -1.333331901677568\n",
      "Step  15  ->  -1.333332760671027\n",
      "Step  16  ->  -1.333333104268411\n",
      "Step  17  ->  -1.3333332417073644\n",
      "Step  18  ->  -1.3333332966829459\n",
      "Step  19  ->  -1.3333333186731782\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for i in range(20):\n",
    "    x = 0.4 * (-1 + 1 * x) + 0.5 * (-1 + 1 * 0) + 0.1 * (1 + 1 * 0)\n",
    "\n",
    "    print(\"Step \",i , \" -> \",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probabilities\n",
    "probs = np.array([\n",
    "        [0, 0.3, 0.4, 0.3, 0],   # s_0 -> s' \n",
    "        [0, 0.0, 0.4, 0.3, 0.3], # s_1 -> s'\n",
    "        [0, 0.2, 0.0, 0.7, 0.1], # s_2 -> s'\n",
    "        [0, 0.1, 0.1, 0.0, 0.8], # s_3 -> s'\n",
    "        [0, 0, 0, 0, 0]     # s_4 -> s'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "R = np.array([\n",
    "        [0, -1, -1, -1, 0], # R(s_0) -> s'\n",
    "        [0, 0, -1, -1, 1],  # R(s_1) -> s'\n",
    "        [0, -1, 0, -1, 1],  # R(s_2) -> s'\n",
    "        [0, -1, -1, 0, 1],  # R(s_3) -> s'\n",
    "        [0, 0, 0, 0, 0]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively apply the Bellman equation for each state and sum across all the probabilities and rewards until we reach convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values and other parameters\n",
    "v = np.zeros(probs.shape[0])\n",
    "v_old = v.copy()\n",
    "gamma = 0.9                     #discounting\n",
    "delta = 1e-5 \n",
    "delta_t = 1\n",
    "dif = 1\n",
    "iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.19208736 -0.46621008 -0.56435147  0.5072491   0.        ]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# For this MDP, we only need about 10 iterations to converge to the value\n",
    "while delta_t > delta:\n",
    "    for s in range(len(probs)):\n",
    "        v[s] = np.sum([\n",
    "                probs[s][sp] * (R[s][sp] + gamma * v_old[sp])\n",
    "                for sp in range(len(probs[s]))\n",
    "            ])\n",
    "    delta_t = np.sum(np.abs(v - v_old))\n",
    "    v_old = v.copy()\n",
    "    iterations = iterations +1 \n",
    "    \n",
    "print(v)\n",
    "\n",
    "print(iterations -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsurprisingly, with the settings above,  s3  is the best state as it has the highest probability to transition to the terminal end state."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6d7ec3969098f20b5bb76372f0ce330742c211d4ffb569c515b08e3cb978d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.datascience': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
